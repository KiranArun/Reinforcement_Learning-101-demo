{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3C-workshop-part2.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "du5kFoxUK9I_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "COPYRIGHT Â© 2018 Kiran Arun <kironni@gmail.com>"
      ]
    },
    {
      "metadata": {
        "id": "xoU4YoAO3qfb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ]
    },
    {
      "metadata": {
        "id": "7ZD_cZhcofAt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 2108
        },
        "outputId": "b1cc7c1c-2aa6-4ce3-cadd-a8a4b71ab79c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525183232589,
          "user_tz": -60,
          "elapsed": 70249,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -r Reinforcement_Learning-101-demo/\n",
        "!git clone https://github.com/KiranArun/Reinforcement_Learning-101-demo.git\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!chmod +x Reinforcement_Learning-101-demo/scripts/setup.sh\n",
        "!./Reinforcement_Learning-101-demo/scripts/setup.sh\n",
        "get_ipython().system_raw('tensorboard --logdir=/content/logdir/ --host=0.0.0.0 --port=6006 &')\n",
        "get_ipython().system_raw('/content/ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print('Tensorboard Link:', json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Reinforcement_Learning-101-demo'...\n",
            "remote: Counting objects: 73, done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 73 (delta 40), reused 47 (delta 21), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n",
            "Getting file: ./A3C-workshop-part2.ipynb\n",
            "Getting file: ./A3C-workshop-part1.ipynb\n",
            "Getting file: ./A3C-load_model.ipynb\n",
            "Getting file: ./A3C-train.ipynb\n",
            "Getting file: ./imgs/RNN_diagram.jpg\n",
            "Getting file: ./imgs/A3C_diagram.jpg\n",
            "Getting file: ./imgs/RL_diagram.jpg\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-5000.meta\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-5000.data-00000-of-00001\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-4500.meta\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-4500.data-00000-of-00001\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-4000.meta\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-4000.data-00000-of-00001\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-3500.meta\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-3500.data-00000-of-00001\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-3000.meta\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-3000.data-00000-of-00001\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-2500.meta\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-2500.data-00000-of-00001\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-2000.meta\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-2000.data-00000-of-00001\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-1500.meta\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-1500.data-00000-of-00001\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-1000.meta\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-1000.data-00000-of-00001\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-500.data-00000-of-00001\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-500.data-00000-of-00001\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-5000.index\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-4500.index\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-4000.index\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-3500.index\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-3000.index\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-2000.index\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-1500.index\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-2500.index\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/checkpoint\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-500.index\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-1000.index\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/model.ckpt-500.meta\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_0/events.out.tfevents.1524684056.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_1/events.out.tfevents.1524684057.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_2/events.out.tfevents.1524684095.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_3/events.out.tfevents.1524684051.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_8/events.out.tfevents.1524684054.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_7/events.out.tfevents.1524684052.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_5/events.out.tfevents.1524684056.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_6/events.out.tfevents.1524684037.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_4/events.out.tfevents.1524684020.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_9/events.out.tfevents.1524684065.f9349e5b9d46\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_11/events.out.tfevents.1524684057.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_10/events.out.tfevents.1524684052.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_13/events.out.tfevents.1524684056.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_12/events.out.tfevents.1524684051.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_14/events.out.tfevents.1524684055.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_17/events.out.tfevents.1524684058.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_15/events.out.tfevents.1524684064.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_18/events.out.tfevents.1524684056.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_16/events.out.tfevents.1524684050.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_19/events.out.tfevents.1524684023.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/main/events.out.tfevents.1524683956.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_22/events.out.tfevents.1524684046.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_23/events.out.tfevents.1524684068.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_21/events.out.tfevents.1524684054.f9349e5b9d46\n",
            "Getting file: ./logdir/run_01-lr_0.0001-nw_24-tmax_50/worker_20/events.out.tfevents.1524684051.f9349e5b9d46\n",
            "Get:1 http://security.ubuntu.com/ubuntu artful-security InRelease [83.2 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu artful InRelease\n",
            "Get:3 http://archive.ubuntu.com/ubuntu artful-updates InRelease [88.7 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu artful-backports InRelease [74.6 kB]\n",
            "Fetched 247 kB in 0s (289 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "All packages are up to date.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.9.1-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.3.4-2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.5)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.3)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.4.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.10.5)\r\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.14.3)\r\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.11.0)\r\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.2)\r\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (2.18.4)\n",
            "Requirement already satisfied: atari-py>=0.1.1; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.1.1)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.0.0)\n",
            "Requirement already satisfied: PyOpenGL; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (3.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2018.4.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2.6)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow; extra == \"atari\"->gym[atari]) (0.45.1)\n",
            "Requirement already satisfied: opencv-contrib-python==3.3.0.9 in /usr/local/lib/python3.6/dist-packages (3.3.0.9)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-contrib-python==3.3.0.9) (1.14.3)\n",
            "Tensorboard Link: http://b828b622.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hHnKTLPp9nzl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning"
      ]
    },
    {
      "metadata": {
        "id": "J5zgEp7u9nzn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is the second part to the reinforcement learning workshop.\n",
        "\n",
        "In this notebook we will learn what A3C is, and how the model is trained.\n",
        "\n",
        "#### Notebook information:\n",
        "- this notebook will train an A3C model and save checkpoints and logs for tensorboard\n",
        "- it will also go through what A3C is, and how it works in our case\n",
        "- the code is based off https://github.com/awjuliani/DeepRL-Agents/blob/master/A3C-Doom.ipynb\n",
        "- I also recommend the blog post corresponding to his code [Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2)"
      ]
    },
    {
      "metadata": {
        "id": "G1re38049nzq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Asynchronous Advantage Actor-Critic (A3C)\n",
        "\n",
        "Asynchronous: Working along side each other but not in time\n",
        "\n",
        "Advantage: How good or bad actions were in terms of the relative returns\n",
        "\n",
        "Actor: The part calculating the probabilities for each action\n",
        "\n",
        "Critic: The part calculating the values for the state\n",
        "\n",
        "![A3C diagram](imgs/A3C_diagram.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "5zsExkKc9nzs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### High Level Overview:\n",
        "\n",
        "1. Create our master/global network\n",
        "2. Create our worker/local networks\n",
        "3. Set workers to work:\n",
        "    - update local network\n",
        "    - interact with environment using predicted actions for a few steps\n",
        "    - calculate gradient using the most recent experiences\n",
        "    - update global network\n",
        "    - if episode is over:\n",
        "        - calculate gradient using the most recent experiences\n",
        "        - update global network\n",
        "        - restart"
      ]
    },
    {
      "metadata": {
        "id": "tcsq7yiY9nzs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "metadata": {
        "id": "clkSZ9449nzu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np                 # our linear algebra library\n",
        "import matplotlib.pyplot as plt    # our plotting library\n",
        "import tensorflow as tf            # our machine learning library\n",
        "\n",
        "import gym                         # the environment library\n",
        "import cv2                         # our image processing library\n",
        "import scipy.signal                # to discount our rewards over time\n",
        "import threading                   # to run multiple workers at the same time\n",
        "\n",
        "import time,os,sys\n",
        "from IPython.display import HTML   # to show videos\n",
        "\n",
        "import A3C_helper_functions as helper # helper functions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2cUFjB0t9nz8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Settings and Hyperparameters\n",
        "\n",
        "**gamma:** discount value, for time step $t$, the reward $k$ steps in the future, $d(r_{t+k})=\\gamma^kr_{t+k}$\n",
        "\n",
        "**state_size:** the input frame size (the image will be unstacked into a vector)\n",
        "\n",
        "**num_actions:** number of possible actions in environment\n",
        "\n",
        "**max_episodes:** number of episodes any worker needs to reach for training to stop (you can also stop it manually whenever)\n",
        "\n",
        "**learning_rate:** learning rate\n",
        "\n",
        "**num_workers:** number of worker networks\n",
        "\n",
        "**start_checkpoint:** You canset it to whatever model checkpoint you want (with the same network and num_workers). the model will start training from here instead of from scratch."
      ]
    },
    {
      "metadata": {
        "id": "eJechNAj9nz9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "gamma = .99           # discount value over time\n",
        "state_size = 84**2    # the size of our input image (unstacked)\n",
        "num_actions = 6       # the number of possible actions to take in environment\n",
        "\n",
        "max_episodes = 10000  # how many episodes for the first worker to reach to stop training\n",
        "learning_rate = 1e-4  # learning rate\n",
        "num_workers = 8       # number of workers\n",
        "tmax = 50             # the number of time steps until an update is made\n",
        "max_grad_norm = 40    # maximum gradient norm\n",
        "\n",
        "# set directories\n",
        "model_root_dir = '/content/'\n",
        "model_logdir = os.path.join(model_root_dir,'logdir/')\n",
        "\n",
        "# model checkpoint to start training from\n",
        "# set to None if training from scratch\n",
        "start_checkpoint = None\n",
        "\n",
        "# our optimizer\n",
        "trainer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=1e-2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "izzdVpYo9nz_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "get the name for this specific training run (so it can be searched on tensorboard)"
      ]
    },
    {
      "metadata": {
        "id": "pddRJtks9n0A",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# get name of the log directory\n",
        "run_logdir = helper.get_logdir(model_logdir,learning_rate,num_workers,tmax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8aizZyNd9n0D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Discount function\n",
        "\n",
        "The further in the future the reward is, the less we want it to contribute to updating the current action.\n",
        "\n",
        "$discount(x,\\gamma)_t = \\sum_{k=0}\\gamma^tx_{t+k}$"
      ]
    },
    {
      "metadata": {
        "id": "Bai0x3f79n0D",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# discounting function used to calculate current + discounted future returns\n",
        "def discount(x, gamma):\n",
        "    return(scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q6j66iVm9n0I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Update worker function\n",
        "\n",
        "This will set the variables from the worker/local network to the master/global networks variables."
      ]
    },
    {
      "metadata": {
        "id": "Po7FbdmR9n0K",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# update worker from global network\n",
        "def update_worker_network(from_scope, to_scope):\n",
        "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
        "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
        "    ops_holder = []\n",
        "    for from_var,to_var in zip(from_vars,to_vars):\n",
        "        ops_holder.append(to_var.assign(from_var))\n",
        "    return(ops_holder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CgP4K3_W9n0O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Network\n",
        "\n",
        "**Inputs:** the input frame(s) (state)\n",
        "- we input the frames unstacked into 1 vector per image\n",
        "- we then put the images back into their image shape\n",
        "- None is the batch size but we only feed 1 frame per forward pass in inference\n",
        "\n",
        "**Conv net:** 2 convolutional layers\n",
        "- this is to analyse the image\n",
        "\n",
        "**Fully Connected:** a single fully connected layer\n",
        "- this is to finnish analysing the image\n",
        "\n",
        "**LSTM:** a Long Short Term Memory cell\n",
        "- we input the output of fc1 and the lstm state\n",
        "- the lstm cell will output an updated lstm state every forward pass, and we'll pass that in next step\n",
        "- this will allow the neural net to make a decision based on not just the current input, but the history (the state)\n",
        "- this goes for all recurrent neural networks (lstm's are a subcatagory)\n",
        "\n",
        "**Policy:** the fully connected output layer\n",
        "- we use a final fully connected layer to out 6 probabilities\n",
        "- the probability represents how good its corresponding action is\n",
        "- we use softmax activation to convert raw outputs to probabilities\n",
        "\n",
        "**Value:** another fully connected output layer\n",
        "- this will predict the value of the current state\n",
        "- this is the same as all future rewards summed together"
      ]
    },
    {
      "metadata": {
        "id": "01gxswf89n0P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Updating the Network\n",
        "\n",
        "We will update the global network using the gradient from the local networks.\n",
        "\n",
        "#### Loss functions:\n",
        "\n",
        "**Value loss:** the difference between predicted total, discounted rewards, and true total, discounted rewards.\n",
        "- The target values are $TV_t = v_{t+K}+\\sum_{k=0}^K\\gamma^kr_{t+k}$\n",
        "- with $v_t =$ the predicted value for time step $t$\n",
        "- $L = \\frac{1}{2}\\sum_{t=0}^T(TV_t - v_t)^2$\n",
        "\n",
        "**Policy loss:** this will use the values and true rewards to change the policy to bias towards better actions\n",
        "- advantages = difference in reward between the true actions taken, and the actions with the highest probability given the current policy (the value)\n",
        "- the advantage > 0, if reward is greater than value\n",
        "- the advantage < 0, if reward is lower than value\n",
        "- when the advantage > 0, the action probabilities will be driven up to decrease the loss\n",
        "    - this means the rewards for the route taken were better than the rewards for the route our model would choose\n",
        "    - and so the proabilities for the route taken should be higher\n",
        "- when the advantage < 0, the action probabilities will be driven down to decrease the loss\n",
        "    - this means the rewards for the route taken were worse than the rewards for the route our model would choose\n",
        "    - and so the proabilities for the route taken should be lower\n",
        "- with $a_t =$ action probability at step $t$ and $d_t =$ advantage as step $t$\n",
        "- $-\\sum_{t=0}^T\\ln(a_t)d_t$\n",
        "    \n",
        "**Entropy:** this will increase as more of the mass of the action mass function (policy) goes on to one action\n",
        "- this makes the action probabilities more even, increasing exploration"
      ]
    },
    {
      "metadata": {
        "id": "7IJdIXAi9n0P",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class AC_Network():\n",
        "    def __init__(self, state_size, num_actions, scope, trainer):\n",
        "        with tf.variable_scope(scope):\n",
        "            \n",
        "            # Inputs\n",
        "            self.inputs = tf.placeholder(shape=[None, state_size], dtype=tf.float32)\n",
        "            self.inputs_reshaped = tf.reshape(self.inputs, shape=[-1, 84, 84, 1])\n",
        "            \n",
        "            # Conv net - image analysis\n",
        "            self.conv1 = tf.layers.conv2d(inputs=self.inputs_reshaped,\n",
        "                                          filters=16,\n",
        "                                          kernel_size=[8, 8],\n",
        "                                          strides=[4, 4],\n",
        "                                          activation=tf.nn.elu,\n",
        "                                          padding='SAME')\n",
        "            self.conv2 = tf.layers.conv2d(inputs=self.conv1,\n",
        "                                          filters=32,\n",
        "                                          kernel_size=[4, 4],\n",
        "                                          strides=[2, 2],\n",
        "                                          activation=tf.nn.elu,\n",
        "                                          padding='SAME')\n",
        "            \n",
        "            # Fully connected - image analysis\n",
        "            self.fc1 = tf.layers.dense(inputs=tf.layers.flatten(self.conv2),\n",
        "                                       units=256,\n",
        "                                       activation=tf.nn.elu)\n",
        "\n",
        "            # LSTM - include information about the history in the predictions\n",
        "            lstm_state_size = 256\n",
        "            lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=lstm_state_size, state_is_tuple=True)\n",
        "            \n",
        "            # create initial state\n",
        "            self.initial_state = (np.zeros([1,lstm_state_size]),np.zeros([1,lstm_state_size]))\n",
        "            \n",
        "            # create state placeholders\n",
        "            c_input_state = tf.placeholder(shape=[1,lstm_state_size], dtype=tf.float32)\n",
        "            h_input_state = tf.placeholder(shape=[1,lstm_state_size], dtype=tf.float32)\n",
        "            self.input_state = tf.nn.rnn_cell.LSTMStateTuple(c_input_state,h_input_state)\n",
        "            \n",
        "            # reformat input data [batch_size, time_steps, features]\n",
        "            # our batch size is always 1\n",
        "            rnn_in = tf.expand_dims(self.fc1, [0])\n",
        "            \n",
        "            # define lstm\n",
        "            lstm_outputs, self.lstm_state = tf.nn.dynamic_rnn(cell=lstm_cell,\n",
        "                                                              inputs=rnn_in,\n",
        "                                                              initial_state=self.input_state,\n",
        "                                                             )\n",
        "            \n",
        "            # reshape the outputs so that shape = [time_steps, lstm_state_size]\n",
        "            # our batch size is always 1, so we can remove it\n",
        "            rnn_out = tf.reshape(lstm_outputs, [-1, lstm_state_size])\n",
        "            \n",
        "                        \n",
        "            # Policy - probabilities of each action\n",
        "            self.policy = tf.layers.dense(inputs=rnn_out,\n",
        "                                          units=num_actions,\n",
        "                                          activation=tf.nn.softmax,\n",
        "                                          bias_initializer=None)\n",
        "            \n",
        "            # Value - total discounted reward given current policy\n",
        "            self.value = tf.layers.dense(inputs=rnn_out,\n",
        "                                         units=1,\n",
        "                                         activation=None,\n",
        "                                         bias_initializer=None)\n",
        "            \n",
        "            # for all worker networks\n",
        "            # this will update the global network using the current workers gradients\n",
        "            if scope != 'global':\n",
        "                \n",
        "                # actions placeholder\n",
        "                self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
        "                self.actions_onehot = tf.one_hot(self.actions, num_actions, dtype=tf.float32)\n",
        "                # the reward of current step + future rewards discounted\n",
        "                self.target_values = tf.placeholder(shape=[None], dtype=tf.float32)\n",
        "                # difference in reward for current and future steps discounted\n",
        "                self.advantages = tf.placeholder(shape=[None], dtype=tf.float32)\n",
        "                \n",
        "                # get probability values of actions\n",
        "                self.responsible_outputs = tf.reduce_sum(self.policy * self.actions_onehot, [1])\n",
        "\n",
        "                # Loss functions\n",
        "                \n",
        "                # Value loss - difference between predicted and true rewards\n",
        "                self.value_loss = 0.5 * tf.reduce_sum(tf.square(self.target_values - tf.reshape(self.value, [-1])))\n",
        "                \n",
        "                # Policy loss -  how good were the rewards compared to what rewards if the actions were taken from the policy\n",
        "                self.policy_loss = -tf.reduce_sum(tf.log(self.responsible_outputs) * self.advantages)\n",
        "                \n",
        "                # Entropy - to make policy values more uniform to increase exploration\n",
        "                self.entropy = - tf.reduce_sum(self.policy * tf.log(self.policy))\n",
        "                \n",
        "                # combine losses\n",
        "                self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy * 0.01\n",
        "\n",
        "                # get local variables\n",
        "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
        "                # calculate gradients of loss in terms of local variables\n",
        "                self.gradients = tf.gradients(self.loss, local_vars)\n",
        "                # clip gradients when norm > 50\n",
        "                # this reduces the values so the norm = 50\n",
        "                grads, self.grad_norms = tf.clip_by_global_norm(self.gradients, max_grad_norm)\n",
        "\n",
        "                # get global variables and perform backpropagation on them\n",
        "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
        "                self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yjljd1t69n0T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Workers\n",
        "\n",
        "In the work function, it will:\n",
        "\n",
        "repeat:\n",
        "- **UPDATE LOCAL VARS:** set the local variables to the global variables\n",
        "- **RESET AND INITIATE THINGS:** reset and initiate environment, episode history/buffer and lstm state\n",
        "\n",
        "- repeat:\n",
        "    - **GET ACTION:** feed state in to neural net and get policy as output (action probabilities)\n",
        "    - **PERFORM ACTION:**, choosing the action from the action probabilitiy distibution (the policy)\n",
        "    - **SAVE TO BUFFER:** record the input state, action, reward, new state and predicted value to history\n",
        "    - **TRAIN:** if buffer length = tmax, call train function\n",
        "        - **TARGET REWARDS:** for time step $t$ = total discounted rewards from after and including $t$\n",
        "        - $TV_t = v_{t+K}+\\sum_{k=0}^K\\gamma^kr_{t+k}$\n",
        "        - **ADVANTAGES:** advantage for time step $t$ = total discounted (reward for step $t$ - predicted reward for step $t$)  from after and including $t$\n",
        "        - $A_t = \\sum_{k=0}^K \\gamma^k(r_{t+k} - (v_{t+k} - \\gamma v_{t+k+1}))$\n",
        "        - **APPLY UPDATE:** update global network using apply grads from above\n",
        "        - **UPDATE LOCAL VARS:** set the local variables to the global variables\n",
        "        \n",
        "- until episode has finnished\n",
        "- **TRAIN:**\n",
        "    - **APPLY UPDATE:** update global network using apply grads from above\n",
        "\n",
        "until max episodes reached"
      ]
    },
    {
      "metadata": {
        "id": "01mkMHfp9n0T",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Worker():\n",
        "    def __init__(self, worker_number, state_size, num_actions, trainer, max_episodes):\n",
        "        \n",
        "        # set up worker\n",
        "        self.worker_name = \"worker_\" + str(worker_number)\n",
        "        self.number = worker_number\n",
        "        self.trainer = trainer\n",
        "        self.max_episodes = max_episodes\n",
        "        self.episodes_history = np.empty([0,2])\n",
        "        \n",
        "        # create a writer for current worker to save episode steps and reward\n",
        "        self.writer = tf.summary.FileWriter(run_logdir+'/'+self.worker_name)\n",
        "        \n",
        "        # create worker network\n",
        "        self.local_AC = AC_Network(state_size, num_actions, self.worker_name, trainer)\n",
        "        # get operations to update local network from global network\n",
        "        self.update_local_ops = update_worker_network('global', self.worker_name)\n",
        "        \n",
        "        # create local environment\n",
        "        self.env = gym.make('SpaceInvaders-v4')\n",
        "        \n",
        "    # function to update global network with workers experience and gradients\n",
        "    def train(self, rollout, sess, gamma, bootstrap_value):\n",
        "        \n",
        "        # seperate history from worker experience buffer (rollout)\n",
        "        rollout = np.array(rollout)\n",
        "        observations = rollout[:,0]\n",
        "        actions = rollout[:,1]\n",
        "        rewards = rollout[:,2]\n",
        "        next_observations = rollout[:,3]\n",
        "        values = rollout[:,5]\n",
        "        \n",
        "        # TARGET REWARDS\n",
        "        # add predicted value to end of rewards\n",
        "        # this is called bootstrapping\n",
        "        # we are combining the networks output and rewards recieved to improve the network\n",
        "        rewards_with_bootstrap = np.asarray(rewards.tolist() + [bootstrap_value])\n",
        "        # total rewards over time discounted\n",
        "        discounted_rewards = discount(rewards_with_bootstrap,gamma)[:-1]\n",
        "        \n",
        "        # ADVANTAGES\n",
        "        # add predicted value to end of predicted values\n",
        "        values_with_bootstrap = np.asarray(values.tolist() + [bootstrap_value])\n",
        "        # reward recieved - reward predicted for time step t\n",
        "        advantages = rewards + gamma * values_with_bootstrap[1:] - values_with_bootstrap[:-1]\n",
        "        # discounted and summed over time\n",
        "        advantages = discount(advantages, gamma)\n",
        "        \n",
        "        # create feed dict\n",
        "        feed_dict = {self.local_AC.target_values:discounted_rewards,\n",
        "                     self.local_AC.inputs:np.vstack(observations),\n",
        "                     self.local_AC.actions:actions,\n",
        "                     self.local_AC.advantages:advantages,\n",
        "                     self.local_AC.input_state[0]:self.batch_lstm_states[0],\n",
        "                     self.local_AC.input_state[1]:self.batch_lstm_states[1]}\n",
        "        \n",
        "        # APPLY UPDATE\n",
        "        # run training step to apply gradients to global network\n",
        "        self.batch_lstm_states,_ = sess.run([self.local_AC.lstm_state,\n",
        "                                             self.local_AC.apply_grads],\n",
        "                                            feed_dict=feed_dict)\n",
        "        \n",
        "    # function to run a worker during training\n",
        "    def work(self, gamma, sess, coord, saver):\n",
        "        \n",
        "        # initiate current number of episodes worker has done\n",
        "        worker_episode_count = 0\n",
        "        print (\"Starting \" + str(self.worker_name))\n",
        "        with sess.as_default(), sess.graph.as_default():\n",
        "            # when coordinator hasn't been requested to stop\n",
        "            # start an episode\n",
        "            while not coord.should_stop():\n",
        "                \n",
        "                \n",
        "                # UPDATE LOCAL VARS\n",
        "                sess.run(self.update_local_ops)\n",
        "                \n",
        "                \n",
        "                # RESET AND INITIATE THINGS\n",
        "                \n",
        "                # initiate buffer and done\n",
        "                episode_buffer = []\n",
        "                d = False\n",
        "                # initiate history to hold episode stats (for analysis)\n",
        "                episode_history = []\n",
        "                \n",
        "                # Reset environment\n",
        "                init_frame = self.env.reset()\n",
        "                # process frame\n",
        "                processed_s = helper.preprocess_frame(init_frame).reshape(1,-1)\n",
        "                # set input frame\n",
        "                s = processed_s\n",
        "                \n",
        "                # Reset lstm state\n",
        "                self.current_lstm_states = self.local_AC.initial_state\n",
        "                self.batch_lstm_states = self.local_AC.initial_state\n",
        "                \n",
        "                # if not done\n",
        "                while d == False:\n",
        "                    \n",
        "                    # GET ACTION\n",
        "                    # get policy, values and new rnn state\n",
        "                    a_dist, v, self.current_lstm_states = sess.run([self.local_AC.policy,\n",
        "                                                                    self.local_AC.value,\n",
        "                                                                    self.local_AC.lstm_state],\n",
        "                                                                   feed_dict={self.local_AC.inputs:s,\n",
        "                                                                              self.local_AC.input_state[0]:self.current_lstm_states[0],\n",
        "                                                                              self.local_AC.input_state[1]:self.current_lstm_states[1]})\n",
        "                    \n",
        "                    # PERFORM ACTION\n",
        "                    # get chosen action from policy action probabilities\n",
        "                    a = np.random.choice(a_dist[0],p=a_dist[0])\n",
        "                    a = np.argmax(a_dist == a)\n",
        "                    # take step in environment\n",
        "                    unprocessed_s1, r, d, info = self.env.step(a)\n",
        "                    \n",
        "                    # process new frame\n",
        "                    if d == False:\n",
        "                        processed_s1 = helper.preprocess_frame(unprocessed_s1).reshape(1,-1)\n",
        "                        # get maximum of current and previous frames\n",
        "                        s1 = np.maximum(processed_s,processed_s1)\n",
        "                    else:\n",
        "                        s1 = s\n",
        "                       \n",
        "                    # SAVE TO BUFFER\n",
        "                    # add to episode buffer and episode values\n",
        "                    episode_buffer.append([s, a, r, s1, d, v[0,0]])\n",
        "                    \n",
        "                    # update new values\n",
        "                    episode_history += [[s, a, r, s1, d]]\n",
        "                    s = s1\n",
        "                    processed_s = processed_s1\n",
        "                    \n",
        "                    # TRAIN\n",
        "                    # when buffer is full but episode hasn't ended\n",
        "                    if len(episode_buffer) == tmax and d != True:\n",
        "                        \n",
        "                        # get current value prediction to use as future rewards\n",
        "                        v1 = sess.run(self.local_AC.value,\n",
        "                                      feed_dict={self.local_AC.inputs:s,\n",
        "                                                 self.local_AC.input_state[0]:self.current_lstm_states[0],\n",
        "                                                 self.local_AC.input_state[1]:self.current_lstm_states[1]})[0,0]\n",
        "                        \n",
        "                        # run train function to update global network with local gradients\n",
        "                        self.train(episode_buffer, sess, gamma, v1)\n",
        "                        \n",
        "                        # reset local buffer\n",
        "                        episode_buffer = []\n",
        "                        \n",
        "                        # UPDATE LOCAL VARS\n",
        "                        sess.run(self.update_local_ops)\n",
        "                    \n",
        "                    # when episode has finished\n",
        "                    if d == True:\n",
        "                        break\n",
        "                    \n",
        "                # TRAIN\n",
        "                # update global network for the last step in episode with remaining buffer\n",
        "                if len(episode_buffer) != 0:\n",
        "                    self.train(episode_buffer, sess, gamma, 0.0)\n",
        "                \n",
        "                # helper function to save episode stats to tensorboard\n",
        "                self.episodes_history,worker_episode_count = helper.end_training_episode(episode_history,\n",
        "                                                                                         self.episodes_history,\n",
        "                                                                                         worker_episode_count,\n",
        "                                                                                         self.worker_name,\n",
        "                                                                                         saver,\n",
        "                                                                                         sess,\n",
        "                                                                                         run_logdir,\n",
        "                                                                                         self.writer)\n",
        "                \n",
        "                # when the first worker reaches max episodes, request a stop\n",
        "                if worker_episode_count == self.max_episodes:\n",
        "                    print('stopping training,', self.worker_name, 'has reached episode', worker_episode_count)\n",
        "                    coord.request_stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pcmt9PhQ9n0Y",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# reset tensorflow graph\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O6XLQ6F-9n0d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Creating global network and workers"
      ]
    },
    {
      "metadata": {
        "id": "Pj4IojFO9n0e",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# create global network\n",
        "master_network = AC_Network(state_size, num_actions, 'global', None)\n",
        "    \n",
        "workers = []\n",
        "# create list of workers\n",
        "for i in range(num_workers):\n",
        "    workers.append(Worker(i, state_size, num_actions, trainer, max_episodes))\n",
        "\n",
        "# define summary writer for main graph\n",
        "main_writer = tf.summary.FileWriter(run_logdir+'/main')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZEG5a2tG9n0g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Initailizing things\n",
        "\n",
        "- Create Tensorflow Session\n",
        "- Create Tensorflow coordinator to handle the threads\n",
        "- Create Tensorflow saver to save open/save checkpoints\n",
        "- Restore model checkpoint if set to do so\n",
        "- Write the session to tensorboard"
      ]
    },
    {
      "metadata": {
        "id": "dJgd_jED9n0h",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# setup tensorflow\n",
        "sess = tf.InteractiveSession()               # session\n",
        "\n",
        "coord = tf.train.Coordinator()               # coordinator for threads\n",
        "saver = tf.train.Saver(max_to_keep=None)     # model saver\n",
        "\n",
        "sess.run(tf.global_variables_initializer())  # initialize variables\n",
        "\n",
        "if start_checkpoint != None:\n",
        "    saver.restore(sess, start_checkpoint)    # choose starting checkpoint\n",
        "    \n",
        "main_writer.add_graph(sess.graph)            # write graph to tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V-iFpqzd9n0j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "- Start workers\n",
        "- Every few minutes, play a test game with global network and log to tensorboard\n",
        "- Save model after training has finnished"
      ]
    },
    {
      "metadata": {
        "id": "LhtqTtBF9n0j",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1463
        },
        "outputId": "0885839e-c648-4516-b32c-7863342c1d05",
        "executionInfo": {
          "status": "error",
          "timestamp": 1525183307669,
          "user_tz": -60,
          "elapsed": 48492,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "worker_threads = []\n",
        "tests = np.empty([0,2])\n",
        "\n",
        "# start workers\n",
        "for worker in workers:\n",
        "    worker_work = lambda: worker.work(gamma, sess, coord, saver)\n",
        "    t = threading.Thread(target=(worker_work))\n",
        "    t.start()\n",
        "    worker_threads.append(t)\n",
        "    time.sleep(0.5)\n",
        "\n",
        "# start tests with master network\n",
        "n = 0\n",
        "while coord.should_stop() != True:\n",
        "    tests = np.append(tests, np.array(helper.test(sess,master_network)).reshape(1,2),axis=0)\n",
        "    summary = tf.Summary()\n",
        "    summary.value.add(tag='test_episode_steps',simple_value=int(tests[-1,0]))\n",
        "    summary.value.add(tag='test_episode_reward',simple_value=int(tests[-1,1]))\n",
        "    main_writer.add_summary(summary, n)\n",
        "    main_writer.flush()\n",
        "    n += 1\n",
        "    time.sleep(180)\n",
        "    \n",
        "# stop and save when threads have all stopped\n",
        "coord.join(worker_threads)\n",
        "print('Saving final model...')\n",
        "saver.save(sess, os.path.join(run_logdir, \"final_model.ckpt\"))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting worker_0\n",
            "Starting worker_1\n",
            "Starting worker_2\n",
            "Starting worker_3\n",
            "Starting worker_4\n",
            "Starting worker_5\n",
            "Starting worker_6\n",
            "Starting worker_7\n",
            "Starting worker_8\n",
            "Starting worker_9\n",
            "Starting worker_10\n",
            "Starting worker_11\n",
            "Starting worker_12\n",
            "Starting worker_13\n",
            "Starting worker_14\n",
            "Starting worker_15\n",
            "worker_7 :  episode: 0 reward: 30.0 actions: [16. 12. 17. 13. 17. 25.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-6bdb349edc61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaster_network\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test_episode_steps'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msimple_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/A3C_helper_functions.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(sess, network)\u001b[0m\n\u001b[1;32m     90\u001b[0m                                                    feed_dict={network.inputs:s,\n\u001b[1;32m     91\u001b[0m                                                       \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcurrent_lstm_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                                                       network.input_state[1]:current_lstm_states[1]})\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mraw_s1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "_qsqNpmK9n0o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you stopped training before letting it finnish, this will:\n",
        "- check for final saved model\n",
        "- if it doesnt exist:\n",
        "    - stop threads\n",
        "    - save final model"
      ]
    },
    {
      "metadata": {
        "id": "-8ampvt59n0p",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# if stopped before it saves and stops threads, do this\n",
        "if os.path.exists(os.path.join(run_logdir,\"final_model.ckpt.index\")) != True:\n",
        "  COPYRIGHT Â© 2018 Kiran Arun <kironni@gmail.com>  coord.request_stop()\n",
        "    coord.join(worker_threads)\n",
        "    print('Saving final model...')\n",
        "    saver.save(sess, os.path.join(run_logdir, \"final_model.ckpt\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xQPDVqjW9n0t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Play and display test game"
      ]
    },
    {
      "metadata": {
        "id": "gx7G_VMK9n0u",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# test game\n",
        "frames = helper.display_test(sess,network)\n",
        "anim = helper.create_gameplay_video(frames,figsize=(5,5),save=False)\n",
        "HTML(anim.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}